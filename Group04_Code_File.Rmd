---
title: "MIS-341 Employee Absenteesim Course Project"
author: "Group4_Section2"
date: "`r Sys.Date()`"
output: 
  html_document:
    toc: true
    theme: readable
editor_options: 
  markdown: 
    wrap: sentence
---

<img src="https://mjaali.com/R-Squad/Assets/Brand/Rsquad%20Project%20Report%20Header.png"/>

### Notes & Remarks

This r markdown project is the work of ***group 04*** students undertaking the course ***MIS-341 : Introduction to Data Analytics*** at **King Fahd University of Petroleum and Minerals** within KFUPM Business school department

-   Kindly press ***`Ctrl + Shift + O`*** to quickly browse through the code
-   Some of the codes are commented, to undo this effect: kindly select the codes in the chunks and press ***`Ctrl + Shift + C`***
-   This project is covered in a highly detailed manner, kindly visit our github repository to get the full in-depth resources of the project [Github](https://github.com/mjaali/R-Squad), it includes the `README` file that has a wider point of view coverage of the case

# 0. Setting the resources

In this first segment, we will cover the essential tasks to do prior to any analysis.
we will prepare the R studio IDE with all the required resources, and organize them in a clean manner.
These duties include the following :

-   Downloading packages used for the project

The packages we will use will be retrieved from the official CRAN repository, they are basically packages designed and created by the community in order to simplify rather complicated codes into clear functions

-   Loading packages

in R studio, packages have to be installed then loaded into the environment.
Therefore, we specified a chunk code for the purpose of loading the packages

-   Setting the Directory

R studio has a built in function of simplifying this task when creating an R project.
But, since this project was shared on the cloud via github, we will reserve chunk of code that will set the directory per member.

-   Importing the Data set

The final step of R preparation is to import in the data for analysis.
We will use clear names throughout our code to make it easier for a reviewer to investigate our analysis and follow along the coding process.

------------------------------------------------------------------------

## 0.a Libraries and Packages

```{r}
#'[ FIRST | we are going to creat a code that will install all codes required for us to do our analysis
# install.packages("tidyverse") # one of the best all-round packages for data processing
# install.packages("writexl")   # a package for reading/importing excel files into R
# install.packages("readxl")    # a package for writing/exporting R files into excel
# install.packages("scales")    # a package used to scale vectors to percentages
# install.packages("psych")     # an analysis package used for regression related statistics
# 
# #' packages installed by MQ
# install.packages("descr")

#'[ SECOND | we load all the packages into R 
library(tidyverse); library(writexl); library(readxl); library(scales); library(psych); library(desc)

```

## 0.b Directory & Import

```{r}
# Abdulrahman:
#'[ Setting the Directory
#setwd( dir = "D:/4. MIS-341/1. Course Work/00. Group Project/MIS341-02_Group 4")
#'[ Loading the File (AJ)
#AbsEmployees <- read_excel("../MIS341-02_Group 4/1. input/AbsEmployees.xlsx", sheet = 1)
# Mohammed Alquraini:
#'[ Setting the Directory (MQ)
# setwd( dir = "C:\\Users\\Lenovo\\Documents\\MIS431- Project\\")
#'[ Loading the File (AG)
AbsEmployees <- read_excel("D:/4. MIS-341/1. Course Work/00. Group Project/MIS341-02_Group 4/1. input/AbsEmployees.xlsx", sheet = 1)
####
# # Loading file - github fetch version (MJ - Github)
# setwd( dir = "/Users/mjaali/webapps/R-Squad/Assets/");
# AbsEmployees <- read_excel("MJ Version - AbsEmployees.xlsx", sheet = 1);
# AbsEmployees <- read_excel("Abdulrahman Version - AbsEmployees.xlsx", sheet = 1);
#duplicating the data set to re-code if necessary
emp  <- AbsEmployees
empR <- emp
#'* NOTE *
#'[ emp  : will house the data ready for analysis ( Whenever I want to go back to my original data before any adjustments, I will use AbsEmployees )
#'[ empR : will house the data archived from emp  (I never delete data, I always archive it in case I need to go back to it for the case of mistakes)
```

------------------------------------------------------------------------

# 1. Data Pre-processing

## 1.a Data Exploration

After browsing through the data and understanding its structure, we defined some cases that need to be covered before proceeding with the analysis.
Instead of deleting the suspected entries, we will create a second data frame that will house the data rejected from the analysis.
Therefore, we will import the `original data set as AbsEmployees`, and then re-assign them into a data analysis set and a rejected set :

> ***emp*** : Is the name of the data analysis set that will contain the data set ready-for-analysis\

> ***empR*** : Is the data set rejected from analysis, and it will include the following : ( Ambiguous store location entries - Duplicated entries - Incorrect Age Entries )

## 1.b Data Cleaning

Extracting the data We have observed that some data entries (employees), have ages below 18, and above 65.Legal age for working in Canada is 18 on average.Through further discussion with our manager, we have dictated that employees with ages out of these bounds will be removed from analysis.Furthermore, some of the entries based on their store location are inconsistent.For example, A store location in a rural area will only have a single cashier.This is clearly the issue of a wrong entry of some scale in the data base.Which has also been extracted out of our analysis

The final case observed are of duplicated entries.
It is uncommon to have multiple employees across different store locations to share the same first & last name, job title and so on.But it is outside the realms of reality to have multiple employees share many indicative features in the data base.Our decision was to also exclude those entries outside of our analysis, to keep our analysis as authentic as possible.

**Note :** `empR` data set will have a column on the right-most specifying why the entry was rejected from analysis.
Kindly refer to it in-case of amendments.

```{r}
#'[ Extracting rejected data to empR data set
# creating new columns that will help with the analysis (will be removed later form the environments and data frame )
empR$dup <- data.frame(paste(empR$Surname, empR$GivenName, empR$Gender, empR$JobTitle, empR$DepartmentName, empR$StoreLocation ))
#'[ Employee Age cleaning ]
# extracting age restrictions
empR_age <- subset(empR, empR$Age < 18 | empR$Age > 65 | empR$Age - empR$LengthService < 18 )
# providing a columns with explanation to why the row was removed from the analysis
empR_age$drop <-  case_when( empR_age$Age < 18 ~ "Under Age",
                             empR_age$Age > 65 ~ "Over Age",
                             empR_age$Age - empR_age$LengthService < 18 ~ "Under Age")
#'[ Archiving duplicated Employee Entries ]
# extracting duplicated entries
empR_dupli <- empR[duplicated(empR$dup), ]
# providing a column with explanation to why the row was removed from the analysis
empR_dupli$drop <- paste("duplicated entry")
# merging archived data
empR <- rbind(empR_age, empR_dupli)
# deleting extra columns and removing variables from environment
empR <- subset(empR, select = -c(dup))
rm(empR_age, empR_dupli)
```

### 1.b.i Cleaning the Ready-for-Analysis Data

```{r}
#'[ removing incorrect entries from the data set, and preparing it for analysis
# creating new columns that will help with the analysis (will be removed later form the environments and data frame )
emp$dup <- data.frame(paste(emp$Surname, emp$GivenName, emp$Gender, emp$JobTitle, emp$DepartmentName, emp$StoreLocation ))
#'[ Deleting Under/over age Employee Entries ]
emp <- emp %>%
  filter(, Age > 18) %>%
  filter(, Age < 65) %>%
  filter(, Age - LengthService > 18)
#'[ Deleting duplicated Employee Entries ]
# deleting duplicate entries
emp <- emp[!duplicated(emp$dup), ]
# deleting excess columns
emp <- subset(emp, select = -c(dup))
```

## 1.c Data Wrangling

In this part, we are concerned with any manipulation required in order to improve the statistical accuracy and quality of our study.
Therefore, after a keen study of the data and the case at hand, we have introduced the following variables :

-   **AbsHoursPerTenure** : $AbsHoursPerTenur=AbsenceHours / LengthService$\
    This variable will be our dependent variable, the feature that is already existent in the data only accounts for the absent hours, regardless of the years of tenure.
    Since it is not statistically feasible to compare the absence of multiple employees with different tenures, it was decided to make a new variable that will take the proportional absence hours per year of service.

-   **EmployeeType** : This new variable will identify which type of employee we are studying, are they a working employee as a cashier or a back-end employee as a finance staff, this will help us identify which employee we can't afford to go absent\

<img src="https://raw.githubusercontent.com/mjaali/R-Squad/main/Assets/Pics/variable%20-%20employee%20type.png?token=GHSAT0AAAAAAB2IXOASCOP7TV7MUGKAAJM4Y46MOMQ"/>

-   **EmployeeSector** : This variable will help us identify absenteeism based on the sector the employee works in, it answers the question of "Are finance employees more likely to go absent in comparison to HR employees?".\

<img src="https://raw.githubusercontent.com/mjaali/R-Squad/main/Assets/Pics/variable%20-%20sector.png?token=GHSAT0AAAAAAB2IXOASFJ5TS2OON7ZM52EMY46MPCQ"/>

```{r}
# (1) creating a proportional Absence hours per year in the company
emp$'AbsHrsPerTenure'   <- (emp$AbsentHours/emp$LengthService)
# (3) adding a variable identifying the type of employee
emp$EmployeeType <- if_else( emp$JobTitle %in% c("Cashier", "Produce Clerk", "Shelf Stocker", "Dairy Person", "Meat Cutter", "Baker"), "Working Employee",
                             if_else(emp$JobTitle %in% c("Store Manager", "Customer Service Manager", "Produce Manager", "Processed Foods Manager", "Dairy Manager", "Meats Manager", "Bakery Manager"), "Work-Manager Employee",
                                     if_else(emp$JobTitle %in% c("Auditor", "Investment Analyst", "Accounting Clerk", "Accounts Payable Clerk", "Accounts Receiveable Clerk", "Trainer", "Compensation Analyst", "Labor Relations Analyst", "Recruiter", "Benefits Admin", "HRIS Analyst", "Systems Analyst", "Corporate Lawyer"), "Staff Employee",
                                             if_else(emp$JobTitle %in% c("Director, Audit", "Director, Investments", "Director, Accounting", "Director, Accounts Payable", "Director, Accounts Receivable", "Director, Training", "Director, Compensation", "Director, Labor Relations", "Director, Recruitment", "Director, Employee Records", "Director, HR Technology"), "Staff-Manager Employee",
                                                     if_else(emp$JobTitle %in% c("VP Finance", "Exec Assistant, Finance", "VP Human Resources", "Exec Assistant, Human Resources", "Exec Assistant, Human Resources", "VP Stores", "Exec Assistant, VP Stores", "CIO", "CEO", "Legal Counsel", "Exec Assistant, Legal Counsel", "CHief Information Officer" ), "Executive Employee",
                                                             emp$JobTitle)))))
# (3) adding a unit categorizing the division of the employee
emp$EmployeeSector <- if_else(emp$JobTitle %in% c("Cashier", "Produce Clerk", "Shelf Stocker", "Dairy Person", "Meat Cutter", "Baker", "Store Manager", "Customer Service Manager", "Produce Manager", "Processed Foods Manager", "Dairy Manager", "Meats Manager", "Bakery Manager"), "Stores Sector",
                      if_else(emp$JobTitle %in% c("Trainer", "Compensation Analyst", "Labor Relations Analyst", "Recruiter", "Benefits Admin", "HRIS Analyst", "Systems Analyst", "Corporate Lawyer", "Director, Training", "Director, Compensation", "Director, Labor Relations", "Director, Recruitment", "Director, Employee Records", "Director, HR Technology"), "Human Resources Sector",
                              if_else(emp$JobTitle %in% c("Auditor", "Investment Analyst", "Accounting Clerk", "Accounts Payable Clerk", "Accounts Receiveable Clerk", "Director, Audit", "Director, Investments", "Director, Accounting", "Director, Accounts Payable", "Director, Accounts Receivable"), "Finance Sector",
                                      "Chief/BoD")))
```

## 1.d Data Classification

after the addition of the variables that we will use for prediction.
We will remove unnecessary variables from the statistical analysis, such variables are within the realms of **EmployeeNumber**, **Surname**, **GivenName**... etc.
Any variable that will not affect the dependent variable will be removed from the data set.

Additionally, we will ensure data variables represent their correct data types.
We will make sure a number variable is identified as `double` instead of `character`.
We have to ensure the data types are assignet correctly in order to create dummy variables and use our predictive models ensuring our results are valid.

```{r}
# removing excess variables and re-ordering the data
emp <- emp %>% 
  select(, "EmployeeNumber", "EmployeeSector", "EmployeeType", "Gender", "JobTitle", "DepartmentName", "Division", "BusinessUnit", "StoreLocation", "Age", "LengthService", "AbsentHours", "AbsHrsPerTenure" )
# assigning the correct data types
emp$EmployeeSector <- as.factor(emp$EmployeeSector)
emp$EmployeeType <- as.factor(emp$EmployeeType)
emp$Gender <- as.factor(emp$Gender)
emp$DepartmentName <- as.factor(emp$DepartmentName)
emp$Division <- as.factor(emp$Division)
emp$BusinessUnit <- as.factor(emp$BusinessUnit)
# the rest of the variables are correctly assigned (as intended)
```

with the above codes in place, we have prepared our data for analysis.
we explored the data for potential incorrect entries, we examined and removed any observation to another data set `empR`.
additionally, we cleaned our main data set, added new variables to be studied, assigned the correct data types and ensured the data will function properly using r language statistical predictive models.
`emp` is ready-for-analysis, and we will proceed to the next part of the report were we clarify how we will conduct our analysis to find the most influencial factors relating to employee absenteeism.

------------------------------------------------------------------------

# 2. Analysis

After we cleaned the data set, we commenced to the next segment in our project, which is the analysis of the data.
the goal of the project is to identify factors that are most effective to absenteeism in the company, highlight them and offer our business recommendations on how to reduce them.
Firstly, to reach that conclusion, we will do a descriptive analysis to understand what is currently happening.
Secondly, we will use our findings from the descriptive analysis and conduct our predictive analysis to objectively project what will most likely to happen if we were to go through our recommendations.

## 2.a Descriptive Statistics

In this segment of the project we will attempt to explore the data, and find correlations in our variables given.
From this analysis, we will identify phenomenons and patterns that will justify the absenteeism causes within the company.

### 2.a.i Absenteeism and Length service

We wanted to test link between the connection between how long an employee has served the company and the absenteesim rate they have.
The logic behind this is the fact that if someone served more in the company, are they more likely or less likely to go absent?

```{r}
#'* a graph that shows the link between Length Service & AbsHrsPerTenure *
#'[ A graph shows the results regardless of the business unit
emp %>%
  ggplot() +
  aes(x=LengthService, y=AbsHrsPerTenure) +             # naming the x,y axis
  geom_point(colour = "violet") +                       # Coloring the dots in the graph
  coord_cartesian(xlim = c(0, 40), ylim = c(0, 100)) +  # setting limits to x,y axis (if we keep the outliers, the graph will be so big that we can't see the relationship)
  theme_minimal()                                       # the theme/style of the plot
#'[ A graph shows the results for Stores business unit
emp %>%
  filter(, BusinessUnit == "Stores") %>% 
  ggplot() +
  aes(x=LengthService, y=AbsHrsPerTenure) +             # naming the x,y axis
  geom_point(colour = "red") +                          # Coloring the dots in the graph
  coord_cartesian(xlim = c(0, 40), ylim = c(0, 100)) +  # setting limits to x,y axis (if we keep the outliers, the graph will be so big that we can't see the relationship)
  theme_minimal()                                       # the theme/style of the plot
#'[ A graph shows the results for Headoffice business unit
emp %>%
  filter(, BusinessUnit == "HeadOffice") %>% 
  ggplot() +
  aes(x=LengthService, y=AbsHrsPerTenure) +             # naming the x,y axis
  geom_point(colour = "blue") +                         # Coloring the dots in the graph
  coord_cartesian(xlim = c(0, 40), ylim = c(0, 100)) +  # setting limits to x,y axis (if we keep the outliers, the graph will be so big that we can't see the relationship)
  theme_minimal()                                       # the theme/style of the plot
```

> findings\

we can observe from the above graphs a great that there is a link between length service and absenteeism.the **purple graph** shows the overall relationship between absent hours per tenure for `all employees`.We observed a pattern and decided to do a deep dive as per business unit, and our findings were informative.the **red graph** shows the same graph but uniquely to `stores`, and the **blue graph** shows the relationship for `head office` employees.we can summize from the graphs that for stores employees, they clutter absenteeism early service years, around 0 \~ 10 years of service.Whereas head office employees distribute their absences over their years service.This information is crucial because it gave us a new point of view to observe patterns, now we are considering the absenteeism culture over the whole business unit.Addtionally, we will start to deep dive the importance of absences in Stores unit.We will do analysis on who we deem most important to not be absent.For example, if a high number of `Cashiers` go absent, customers will bottleneck on checkout, jeopardizing the in-store customer experience of the store.

### 2.a.ii Absenteeism and Age

The one the most logical correlations is the age of the employee and how it affects the absenteeism rate they incur.
This analysis answers the questions of *"Is the employee more likely to be absent the older they are? does age affect absenteeism?"*

```{r}
#'* a graph that shows the link between Age and Absence Hours Per Tenure                                                                                                                                       *
#'[ A graph shows the results for the relationship between age & absHrsPerTenure
emp %>%
  ggplot() +
  geom_point(aes(x = Age,y = AbsHrsPerTenure), colour = "purple") + # setting the aesthetics for the points + defining xy axis
  coord_cartesian(xlim = c(15, 60), ylim = c(0, 100)) +             # limiting the xy axis
  theme_minimal()                                                   # setting the theme for the graph
#'[ A graph shows the results for the relationship between age & absHrsPerTenure
emp %>%
  ggplot() +
  geom_point(aes(x = Age,y = AbsHrsPerTenure), colour = "green") +    # setting the aesthetics for the points + defining xy axis
  coord_cartesian(xlim = c(15, 60), ylim = c(-20, 100)) +             # limiting the xy axis
  geom_smooth(aes(x = Age,y = AbsHrsPerTenure) ,method = 'lm') +      # drawing a linear model line in the graph
  theme_minimal()                                                     # setting the theme for the graph
```

> findings

the findings of these graphs will shape our decision for the report.
We can observe a very interesting phenomena when plotting age and absence hours per tenure, the graph shows how the absence hours per tenure increases referring to the age.Starting approximately the age 25, absenteeism increases around 0.9 hours.With this observation we can deduce that as for now, this is the strongest correlation we were able to find thus far.We will keep this graph in mind whilst making our recommendation, as well as explore it in the point of view of its effects per job title.

------------------------------------------------------------------------

# 3. Predictive Statistics

To begin with, we will try to build our model using AbsentHours to be our dependent variable.

Firstly, We will do our correlation matrix to determine the correlation between the independent variables and our response variable, but since our data contains numeric and nonnumerical variables, we will do the correlation matrix manually.

The first step will be defining the model matrix for each variable

```{r}
m_V1 <- model.matrix(~ AbsentHours - 1, emp)
m_V2 <- model.matrix(~ Gender - 1, emp)
m_VJobTitle <- model.matrix(~ JobTitle - 1, emp)
m_VDepartmentName <- model.matrix(~ DepartmentName - 1, emp)
m_V5 <- model.matrix(~ StoreLocation - 1, emp)
m_VDivision <- model.matrix(~ Division - 1, emp)
m_VBusinessUnit <- model.matrix(~ BusinessUnit - 1, emp)
m_V8 <- model.matrix(~ Age - 1, emp)
m_V9 <- model.matrix(~ LengthService - 1, emp)
m_V10 <- model.matrix(~ EmployeeSector - 1, emp)
m_V11 <- model.matrix(~ EmployeeType - 1, emp)
m_V12 <- model.matrix(~ AbsHrsPerTenure - 1, emp)
```

Now we will do our correlation matrix between our response variable and other variables in the dataset.

```{r}
cor(m_V1, m_V2)
cor(m_V1, m_VJobTitle)
cor(m_V1, m_VDepartmentName) 
cor(m_V1, m_V5)
cor(m_V1, m_VDivision)
cor(m_V1, m_VBusinessUnit) 
cor(m_V1, m_V8)
cor(m_V1, m_V9)
cor(m_V1, m_V10) 
cor(m_V1, m_V11)
cor(m_V1, m_V12)
rm(m_V1, m_V2, m_V5, m_V8, m_V9, m_V10, m_V11, m_V12 )
```

After computing the correlation between our response variable and other variables, we will do tests of the correlations between some variables to test our assumptions of having multicollinearity between some variables.
The first test will be between BusinessUnit and Division.

```{r}
cor(m_VDivision, m_VBusinessUnit) 
```

As we can see, we have a significant 100% correlation between DivisionStores and (BusinessUnitHeadOffice and BusinessUnitStores), which indicates that the presence of the two variables in our model will result in multicollinearity issues.
Therefore, our optimal solution is to remove one of the two variables.

Now, we will test the correlation between JubTitle and DepartmentName.

```{r}
cor(m_VJobTitle, m_VDepartmentName)
rm(m_VJobTitle, m_VDepartmentName, m_VDivision, m_VBusinessUnit)
```

Based on the obtained results, we can conclude that the presence of these two variables will create a problem of multicollinearity, due to the high correlation (more than 95%) between some of the observations.
Therefore, our optimal solution is to remove one of the two variables

For the other variables, there is no need for testing the correlation between them, due to that it seems that there is no significant correlation between them, but if a high correlation was percent, R will find it and solve the problem.

After computing the correlation matrix, we will try to run models for each variable to determine which variable has the highest impact.

for our individual model we will use (emp) as our dataset.

```{r}
model01 <- lm(AbsentHours ~Gender, data = emp)
summary(model01)$r.squared
# R-squared of model1 is 0.01436839
model02 <- lm(AbsentHours ~JobTitle, data = emp)
summary(model02)$r.squared
# R-squared of model02 is 0.006038185
model03 <- lm(AbsentHours ~DepartmentName, data = emp)
summary(model03)$r.squared
# R-squared of model03 is 0.003656944
model04 <- lm(AbsentHours ~StoreLocation, data = emp)
summary(model04)$r.squared
# R-squared of model04 is 0.004578336
model05 <- lm(AbsentHours ~Division, data = emp)
summary(model05)$r.squared
# R-squared of model05 is 0.001522046
model06 <- lm(AbsentHours ~BusinessUnit, data = emp)
summary(model06)$r.squared
# R-squared of model06 is 0.001057944
model07 <- lm(AbsentHours ~Age, data = emp)
summary(model07)$r.squared
# R-squared of model07 is 0.6833127
model08 <- lm(AbsentHours ~LengthService, data = emp)
summary(model08)$r.squared
# R-squared of model08 is 0.0005958677
model09 <- lm(AbsentHours ~EmployeeSector, data = emp)
summary(model09)$r.squared
# R-squared of model09 is 0.001376249
model10 <- lm(AbsentHours ~EmployeeType, data = emp)
summary(model10)$r.squared
# R-squared of model10 is 0.001526995
model11 <- lm(AbsentHours ~AbsHrsPerTenure, data = emp)
summary(model11)$r.squared
# R-squared of model11 is 0.1665376
rm(model01, model02, model03, model04, model05, model06, model07, model08, model09, model10, model11)
```

From the obtained result we can see that the variable that has the highest impact on our response variable is (Age) with R-squared of 0.6833127

Now, we will run a model with all variables.

```{r}
model12 <- lm(AbsentHours ~., data = emp)
# ANOVA table will be applied on the model
anova(model12)
# R-squared will be computed
summary(model12)$r.squared
```

As we can observe, the R-squared for the full model came out to be 0.7407666

Additionally, in the full model, 8 variables are significant, therefore, we will drop the insignificant variables and run the model again.

```{r}
model12 <- lm(AbsentHours ~ EmployeeSector+ EmployeeType+ Gender+ JobTitle+ StoreLocation + Age+ LengthService + AbsHrsPerTenure, data = emp)
# ANOVA table will be applied on the model
anova(model12)
# R-squared will be computed
summary(model12)$r.squared
rm(model12)
```

Based on the obtained results, we can observe that the R-squared has not changed after deleting the insignificant variables.
Therefore, the present of the insignificant variables may not affect the R-squared, but it can cause other problems related to our model, such as multicollinearity or outliers.

After computing the models, we will create a new dataset to implement our First model based on the variables that we believe it has an impact on our response variable.

*(PredData)* refers to the data that we are going to use in building our new model.

```{r}
PredData <- emp
PredData = subset(PredData, select = -c(DepartmentName, Division) )
```

We have deleted the listed variables to implement our first model because of the below reasons:

*DepartmentName:* Because it is related to the JobTitle, therefore, if we include it in the model a multicollinearity problem will rise.
*Note:* we can do the opposite but we will do it this way at the moment until we find our optimal model.
*Division:* Because it is related to the business unit, therefore, if we include it in the model a multicollinearity problem will rise.
*Note:* we can do the opposite but we will do it this way at the moment until we find our optimal model.

```{r}
model13 <- lm(AbsentHours ~., data = PredData)
anova(model13)
summary(model13)$r.squared
rm(model13)
```

According to the results obtained using this model, it shows that there is a variable (EmployeeNumber) that is insignificant at alpha = 0.05

R-squared is 0.7407666

Now, we will try some advanced methods for building our optimal model, listed below:

1- Backward elimination

2- Forward Stepwise

------------------------------------------------------------------------

Backward elimination method:

*BackwardData* represents the data that we are going to use to implement our model using the Backward Elimination method.

*BackwardModel* represents our model using the backward elimination method

```{r}
BackwardData <- emp
BackwardModel <- lm(AbsentHours ~., data = BackwardData)
BackwardModel <- step(BackwardModel, direction = "backward", trace=FALSE ) 
# ANOVA table will be shown for the model
anova(BackwardModel)
# R-squared of the model will be applied
summary(BackwardModel)$r.squared
#we will view our model of backward elemination method.
BackwardModel$coefficients
```

As we can see, after using the Backward Elimination method, we have defined five variables to be in our model and all of them are significant at alpha = 0.05

R-squared is 0.7391996

Now, we will use the Forward stepwise regression method

Forward stepwise method

*Stipwisedata* represents the data that we are going to use to implement our model using the Forward stepwise method.

To implement our forward stepwise method we should follow the folowing steps:

1- we will define our intercept-only model

2- we will define a model with all predictors

3- we will perform forward stepwise regression

```{r}
Stipwisedata <- emp
intercept_only <- lm(AbsentHours~ 1, data=Stipwisedata)
#Secondly, we will define a model with all predictors
FullModel <- lm(AbsentHours ~ ., data=Stipwisedata)
# Finally, we wil perform forward stepwise regression
StipwiseModel <- step(intercept_only, direction='forward', scope=formula(FullModel), trace=0)
#Now, we will view the results of forward stepwise regression
anova(StipwiseModel)
summary(StipwiseModel)$r.squared
```

Using forward Stepwise method, we have obtained five variables to be used for our model, but one of these variables (LengthService) is insignificant at alpha = 0.05, therefore, it will be dropped from the model and we will be using just the other four variables which are (Age, Gender, AbsHrsPerTenure, JobTitle).

```{r}
StipwiseModel <- lm(AbsentHours ~ (Age +Gender+ AbsHrsPerTenure+ JobTitle), data = emp)
anova(StipwiseModel)
summary(StipwiseModel)$r.squared
```

After deleting the insignificant variable, the model becomes significant at alpha = 0.05 and can be used in our regression.

R-squared is 0.7391996

As we can see, using these two approaches (Backward elimination, Forward stepwise), we got the same results for the R-squared approach!
Therefore, our optimal model using (AbsentHours) as a response variable will be the model that was obtained using the Forward stepwise approach, as it has a lower number of variables than the Backward Elimination approach with the same confections of determination.
Additionally, although our full model is having higher R-squared than Forward Stepwise, the difference between the R-squared is insignificant.
furthermore, in the full model, we have 8 variables while in this model we have just four which will minimize our effort when we apply the regression.

------------------------------------------------------------------------

Now, we will try to build our model using AbsHrsPerTenure to be our dependent variable.

Firstly, We will do our correlation matrix to determine the correlation between the independent variables and our response variable, but since our data contains numeric and nonnumerical variables, we will do the correlation matrix manually.

```{r}
m_V13 <- model.matrix(~ AbsHrsPerTenure - 1, emp)
m_V14 <- model.matrix(~ Gender - 1, emp)
m_VJobTitle <- model.matrix(~ JobTitle - 1, emp)
m_VDepartmentName <- model.matrix(~ DepartmentName - 1, emp)
m_V17 <- model.matrix(~ StoreLocation - 1, emp)
m_VDivision <- model.matrix(~ Division - 1, emp)
m_VBusinessUnit <- model.matrix(~ BusinessUnit - 1, emp)
m_V20 <- model.matrix(~ Age - 1, emp)
m_V21 <- model.matrix(~ LengthService - 1, emp)
m_V22 <- model.matrix(~ EmployeeSector - 1, emp)
m_V23 <- model.matrix(~ EmployeeType - 1, emp)
m_V24 <- model.matrix(~ AbsentHours - 1, emp)
```

Now we will do our correlation matrix between our response variable and other variables in the dataset.

```{r}
cor(m_V13, m_V14)
cor(m_V13, m_VJobTitle)
cor(m_V13, m_VDepartmentName) 
cor(m_V13, m_V17)
cor(m_V13, m_VDivision)
cor(m_V13, m_VBusinessUnit) 
cor(m_V13, m_V20)
cor(m_V13, m_V21)
cor(m_V13, m_V22) 
cor(m_V13, m_V23)
cor(m_V13, m_V24)
rm(m_V13, m_V14, m_V17, m_V20, m_V21, m_V22, m_V23, m_V24 )
```

After computing the correlation between our response variable and other variables, we can observe that the correlation between the variables and our response variable is small, which indicates that our response variable (AbsHrsPerTenure) may not be ideal.

Now, we will test the correlations between some variables to prove our assumptions of having multicollinearity between some variables.

First test will be between BusinessUnit and Division.

```{r}
cor(m_VDivision, m_VBusinessUnit) 
```

As we can see, we have a significant 100% correlation between DivisionStores and (BusinessUnitHeadOffice and BusinessUnitStores), which indicates that the presence of the two variables in our model will result in multicollinearity issues.
Therefore, our optimal solution is to remove one of the two variables.

------------------------------------------------------------------------

Now, we will test the correlation between JubTitle and DepartmentName.

```{r}
cor(m_VJobTitle, m_VDepartmentName)
rm(m_VJobTitle, m_VDepartmentName, m_VDivision, m_VBusinessUnit)
```

Based on the obtained results, we can conclude that the presence of these two variables will create a problem of multicollinearity, due to the high correlation (more than 95%) between some of the observations.
Therefore, our optimal solution is to remove one of the two variables.

For the other variables, there is no need for testing the correlation between them, due to that it seems that there is no significant correlation between them, but if a high correlation was percent, R will find it and alert us of the problem.

After computing the correlation matrix, we will try to run models for each variable to determine which variable has the highest impact.

for our individual models, we will use (emp) as our dataset.

```{r}
model17 <- lm(AbsHrsPerTenure ~Gender, data = emp)
summary(model17)$r.squared
# R-squared of model17 is 0.0007618371
model18 <- lm(AbsHrsPerTenure ~JobTitle, data = emp)
summary(model18)$r.squared
# R-squared of model18 is 0.003039255
model19 <- lm(AbsHrsPerTenure ~DepartmentName, data = emp)
summary(model19)$r.squared
# R-squared of model19 is 0.002490742
model20 <- lm(AbsHrsPerTenure ~StoreLocation, data = emp)
summary(model20)$r.squared
# R-squared of model20 is 0.001903145
model21 <- lm(AbsHrsPerTenure ~Division, data = emp)
summary(model21)$r.squared
# R-squared of model21 is 0.001381611
model22 <- lm(AbsHrsPerTenure ~BusinessUnit, data = emp)
summary(model22)$r.squared
# R-squared of model22 is 0.0009512285
model23 <- lm(AbsHrsPerTenure ~Age, data = emp)
summary(model23)$r.squared
# R-squared of model23 is 0.112144
model24 <- lm(AbsHrsPerTenure ~LengthService, data = emp)
summary(model24)$r.squared
# R-squared of model24 is 0.05324702
model25 <- lm(AbsHrsPerTenure ~EmployeeSector, data = emp)
summary(model25)$r.squared
# R-squared of model25 is 0.001380237
model26 <- lm(AbsHrsPerTenure ~EmployeeType, data = emp)
summary(model26)$r.squared
# R-squared of model26 is 0.001101277
model27 <- lm(AbsHrsPerTenure ~AbsentHours, data = emp)
summary(model27)$r.squared
# R-squared of model27 is 0.1665376
rm(model17, model18, model19, model20, model21, model22, model23, model24, model25, model26, model27)
```

From the obtained result we can see that the variable that has the highest impact on the response variable is *AbsentHours* and then *Age*.

------------------------------------------------------------------------

Now, we will run a model with all variables.

```{r}
model28 <- lm(AbsHrsPerTenure ~., data = emp)
anova(model28)
summary(model28)$r.squared
rm(model28)
```

As it is shown, the R-squared for the full model is very small, therefore, our response variable for this regression may not be ideal.

After computing the models, we will create a new dataset to implement our First model based on the variables that we believe have a high impact on our response variable

*(PredData2)* refers to the data that we are going to use in building our new model.

```{r}
PredData2 <- emp
PredData2 = subset(PredData2, select = -c(DepartmentName, Division) )
```

We have deleted the listed variables to implement our first model because of the below reasons:

*DepartmentName:* Because it is related to the JobTitle, therefore, if we include it in the model a multicollinearity problem will rise.

*Note:* we can do the opposite but we will do it this way at the moment until we find our optimal model.

*Division:* Because it is related to the BusinessUnit, therefore, if we include it in the model a multicollinearity problem will rise.

*Note:* we can do the opposite but we will do it this way at the moment until we find our optimal model.

```{r}
model29 <- lm(AbsHrsPerTenure ~., data = PredData2)
anova(model29)
summary(model29)$r.squared
rm(model29)
```

According to the results obtained using this model, it shows that there are some variables that are insignificant, As a result, some observations are having NA values, to reduce them to a minimum, we will try some advanced methods of modeling.

------------------------------------------------------------------------

Now, we will try some advanced methods for building our optimal model, listed below:

1- Backward elimination

2 - Forward Stepwise

Backward elimination method:

*BackwardData2* represents the data that we are going to use to implement our model using the backward elimination method

```{r}
BackwardData2 <- emp
BackwardModel2 <- lm(AbsHrsPerTenure ~., data = BackwardData2)
BackwardModel2 <- step(BackwardModel2, direction = "backward", trace=FALSE ) 
#Now, we will view the results of Backward elemination regression
anova(BackwardModel2)
#Finally, we will view our model of backward elemination method.
summary(BackwardModel2)$r.squared
```

As we can see, after using the Backward Elimination method, our variables have been reduced to four, in addition, one of these four is insignificant (JobTitle) based on the P-value approach at alpha = 0.05.

Also, R-squared has been found out to be 0.2362546

------------------------------------------------------------------------

Here, we will use the Forward stepwise regression method

Forward stepwise method

*Stipwisedata2* represents the data that we are going to use to implement our model using the Forward stepwise method.

To implement our forward stepwise method we should follow the folowing steps:

1- we will define our intercept-only model

2- we will define a model with all predictors

3- we will perform forward stepwise regression

```{r}
Stipwisedata2 <- emp
#Firstly, we will define our intercept-only model
intercept_only2 <- lm(AbsHrsPerTenure~ 1, data=Stipwisedata2)
#Secondly, we will define a model with all predictors
FullModel2 <- lm(AbsHrsPerTenure ~ ., data=Stipwisedata2)
# Finally, we wil perform forward stepwise regression
StepwiseModel2 <- step(intercept_only2, direction='forward', scope=formula(FullModel2), trace=0)
#Now, we will view the results of forward stepwise regression
anova(StepwiseModel2)
#At the end, we will view our final model of forward stepwise method.
StepwiseModel2$coefficients
summary(StepwiseModel2)$r.squared
 
rm(PredData, PredData2, Stipwisedata2,BackwardData,BackwardData2)
     
```

As we can see, we have defined in this model 4 variables to be added to our model, and all of them are significant at alpha = 0.05.

Also, R-squared came out to be 0.2321445

------------------------------------------------------------------------

As we can see, using these two approaches (Backward elimination, Forward stepwise), we got defferint results.
Furthermore, the confession of determination is very small for the two methods.

------------------------------------------------------------------------

To conclude, our optimal model will be the one we obtained using the Forward stepwise method with *AbsentHours* as a response variable.

Our final model will be:

```{r}
#
StipwiseModel$coefficients
rm(intercept_only, intercept_only2, Stipwisedata)
```

------------------------------------------------------------------------

# 4. Recommendation

The Cashier has the highest Absenteeism Culture level.
In the city of Vancouver, the age `24.51` y/o and beyond is where Absenteeism hours escalate with a strong correlation for both males and females at a `slope of 0.9395x`.
Since our x-intercept is `24.51`, or in other words, Cashiers below the age of 24.51 have no absenteeism hours `(0 Hrs / year)`.
The avg.
tenure of the Cashier is `4.58 years`.
A policy to hire Cashiers of a maximum of `20 y/o` will ensure most cashiers will clear out of the company before they turn 25 and reduce overall company absenteeism significantly.
However, we have to validate if these young cashiers are full-time or part-time in order to control the cost associated with high-frequency and high-volume recruiting, plus investigate the compensation increase as a result, along with the total supply of such type of workers.
